{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Herramientas de Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Modelos (empezaremos con uno simple y uno más complejo)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Librerías de Explicabilidad (XAI)\n",
    "import shap\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Configuración para que las visualizaciones de SHAP se muestren en el notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c82007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los datos desde el archivo CSV\n",
    "file_path = 'dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Renombrar la columna target\n",
    "if '`' in df.columns:\n",
    "    df.rename(columns={'`': 'target_variable'}, inplace=True)\n",
    "\n",
    "# Definimos las columnas a eliminar\n",
    "features_to_remove = []\n",
    "\n",
    "# Definimos las columnas que SÍ usaremos\n",
    "# (todas las columnas menos 'id', 'target_variable' Y las que queremos eliminar)\n",
    "features_finales = [\n",
    "    col for col in df.columns \n",
    "    if col not in ['id', 'target_variable'] + features_to_remove\n",
    "]\n",
    "\n",
    "# Definimos el target\n",
    "target = 'target_variable'\n",
    "\n",
    "print(f\"Features a eliminar (ruido): {features_to_remove}\")\n",
    "print(f\"Número de features finales: {len(features_finales)}\")\n",
    "print(\"Features que se usarán para entrenar:\")\n",
    "print(features_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a035e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'X_final' solo contiene las columnas que hemos seleccionado\n",
    "X_final = df[features_finales]\n",
    "\n",
    "# 'y_final' es nuestro target\n",
    "y_final = df[target]\n",
    "\n",
    "print(f\"Forma de X_final: {X_final.shape}\")\n",
    "print(f\"Forma de y_final: {y_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4852f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos: 80% para entrenamiento, 20% para prueba\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"Forma de X_train_final: {X_train_final.shape}\")\n",
    "print(f\"Forma de X_test_final: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # Importar el nuevo modelo\n",
    "\n",
    "# 1. Inicializar el modelo Random Forest\n",
    "# random_state=42 para resultados consistentes\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 2. Entrenar el modelo con los mismos datos de entrenamiento\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# 3. Hacer predicciones en el conjunto de test\n",
    "y_pred_rf = rf_model.predict(X_test_final)\n",
    "\n",
    "# 4. Calcular el F1-Score\n",
    "f1_rf = f1_score(y_test_final, y_pred_rf)\n",
    "\n",
    "print(f\"--- Resultados del Modelo (Random Forest) ---\")\n",
    "print(f\"F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "# 5. Ver el reporte completo\n",
    "print(\"\\nReporte de Clasificación (Random Forest):\")\n",
    "print(classification_report(y_test_final, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importancia de Features (Incorporada en el modelo)\n",
    "print(\"--- Insights Globales (Model-Based) ---\")\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': features_finales,  # Usar la lista de features finales\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance_df.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Features (Importancia de Gradient Boosting)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---\n",
    "# 2. Explicabilidad con SHAP (Más Robusto)\n",
    "print(\"\\n--- Insights Globales (SHAP) ---\")\n",
    "\n",
    "# Calculamos los valores SHAP\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test_final)\n",
    "\n",
    "# --- SHAP Summary Plot (Beeswarm) ---\n",
    "print(\"Generando SHAP Summary Plot (Beeswarm)...\")\n",
    "\n",
    "# Este 'if' comprueba el tipo de shap_values y ejecuta el código correcto\n",
    "if isinstance(shap_values, list):\n",
    "    print(\"Diagnóstico: shap_values es una LISTA. Trazando para la Clase 1 ('Won').\")\n",
    "    shap.summary_plot(\n",
    "        shap_values[1],  # Usamos el índice [1] para la clase \"Won\"\n",
    "        X_test_final, \n",
    "        feature_names=features_finales,\n",
    "        show=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Diagnóstico: shap_values es una MATRIZ ÚNICA. Trazando directamente.\")\n",
    "    shap.summary_plot(\n",
    "        shap_values,  # Pasamos la matriz entera directamente\n",
    "        X_test_final, \n",
    "        feature_names=features_finales,\n",
    "        show=False\n",
    "    )\n",
    "        \n",
    "plt.title(\"SHAP Summary Plot (Impacto en la predicción)\")\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP Bar Plot (Importancia media) ---\n",
    "print(\"\\nGenerando SHAP Bar Plot (Importancia media)...\")\n",
    "\n",
    "# Repetimos la misma lógica para el gráfico de barras\n",
    "if isinstance(shap_values, list):\n",
    "    print(\"Diagnóstico: shap_values es una LISTA. Trazando para la Clase 1 ('Won').\")\n",
    "    shap.summary_plot(\n",
    "        shap_values[1], # Usamos el índice [1]\n",
    "        X_test_final, \n",
    "        feature_names=features_finales, \n",
    "        plot_type=\"bar\",\n",
    "        show=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Diagnóstico: shap_values es una MATRIZ ÚNICA. Trazando directamente.\")\n",
    "    shap.summary_plot(\n",
    "        shap_values, # Pasamos la matriz entera directamente\n",
    "        X_test_final, \n",
    "        feature_names=features_finales, \n",
    "        plot_type=\"bar\",\n",
    "        show=False\n",
    "    )\n",
    "        \n",
    "plt.title(\"Importancia Media de Features (SHAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Insights de Features (Partial Dependence Plots) ---\")\n",
    "\n",
    "# Identifica las features más importantes del Bloque 5\n",
    "features_to_plot = feature_importance_df['feature'].head(3).tolist()\n",
    "\n",
    "# Asegurarse de que tenemos features para graficar\n",
    "if not features_to_plot:\n",
    "    print(\"No se identificaron features importantes. Saltando PDP.\")\n",
    "else:\n",
    "    print(f\"Generando PDP para: {features_to_plot}\")\n",
    "    \n",
    "    # Crear los gráficos\n",
    "    fig, ax = plt.subplots(figsize=(15, 5), ncols=len(features_to_plot))\n",
    "    \n",
    "    # Si solo es una feature, ax no es un array, así que lo ajustamos\n",
    "    if len(features_to_plot) == 1:\n",
    "        ax = [ax]\n",
    "        \n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        rf_model,\n",
    "        X_train_final,\n",
    "        features_to_plot,\n",
    "        ax=ax,\n",
    "        grid_resolution=20 # Número de puntos a calcular en el gráfico\n",
    "    )\n",
    "    \n",
    "    fig.suptitle('Partial Dependence Plots (Efecto promedio de la feature en la predicción)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
